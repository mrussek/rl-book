{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eligibility Traces\n",
    "$$\n",
    "\\newcommand{\\td}[1]{\\text{TD}(#1)}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\hv}[2]{\\hat{v}(S_{#1}, \\w_#2)}\n",
    "$$\n",
    "Eligibility traces are one of the most frequently used techniques in Reinforcement Learning.\n",
    "\n",
    "Like $n$-step learning, they also form a continuum between Monte Carlo estimation and TD learning. Monte Carlo methods correspond to $\\lambda = 1$ while TD learning corresponds to $\\lambda = 0$.\n",
    "\n",
    "Eligibility traces are implemented with a vector $\\mathbf{z} \\in \\mathbb{R}^d$ with same dimensionality as the weight vector $\\mathbf{w} \\in \\mathbb{R}^d$. This vector is known as the _eligibility trace_ and roughly corresponds to how recently a particular feature hsa been observed. The trace-decay parameter $\\lambda$ determines how fast the $\\mathbf{z}$ vector decays.\n",
    "\n",
    "One advantage of eligibility traces over $n$-step learning is their computational complexity. Only a single eligibility trace vector needs to be stored as opposed to $n$ feature vectors.\n",
    "\n",
    "Another advantage is that eligibility traces allow for immediate online learning, whereas $n$-step learning requires $n$ subsequent steps to be made before an update will be processed.\n",
    "\n",
    "Eligibility traces provide an example of a _backwards view_ algorithm, where each new observed state is used to perform updates based on earlier state visitations, as opposed to the _forwards view_, where a state value is updated based on the future results acquired subsequent to that state.\n",
    "\n",
    "## 12.1 The $\\lambda$-return\n",
    "\n",
    "We know that $n$-step TD updates targeting\n",
    "\n",
    "$$\n",
    "G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\dots \\gamma^{n - 1}R_{t + n} + \\gamma^n \\hat{v}(S_{t+n}, \\mathbf{w}_{t+n-1})\n",
    "$$\n",
    "\n",
    "eventually converge to true values. However, we note that any _average_ across $n$-step targets would also converge. For example, the update with target\n",
    "\n",
    "$$\n",
    "G_t = \\frac{1}{2}G_{t:t+2} + \\frac{1}{2}G_{t:t+4}\n",
    "$$\n",
    "\n",
    "should also converge.\n",
    "\n",
    "As long as the weights on the $n$-step returns are positive and sum to $1$, any such convex combination is also a valid update target.\n",
    "\n",
    "Averaging different updates is known as _compound updating_.\n",
    "\n",
    "$\\td{\\lambda}$ can be thought of a compounding $n$-step updates for all values of $n$, where we weight each update by $(1 - \\lambda)\\lambda^n$.\n",
    "\n",
    "The target for $\\td{\\lambda}$ can be defined written as\n",
    "\n",
    "$$\n",
    "G_t^\\lambda = (1-\\lambda)\\sum_{n=1}^\\infty{\\lambda^{n-1}G_{t:t+n}}\n",
    "$$\n",
    "\n",
    "in the continuing case, and as\n",
    "\n",
    "$$\n",
    "G_t^\\lambda = (1-\\lambda)\\sum_{n=1}^{T-t-1}{\\lambda^{n-1}G_{t:t+n}} + \\lambda^{T-t-1}G_t\n",
    "$$\n",
    "\n",
    "in the episodic case with $T$ the time of episode termination.\n",
    "\n",
    "In the episodic case, note that when $\\lambda = 1$, we have $G_t^\\lambda = G_t$, which is the update for Monte Carlo, and when $\\lambda = 0$, we have that $G_t^\\lambda = G_{t:t+1}$, which is the update for $\\td{0}$.\n",
    "\n",
    "> _Exercise 12.1_ Just as the return be written recursively in terms of the first reward and itself one-step later (3.9), so can the $\\lambda$-return. Derive the analogous recursive relationship from (12.2) and (12.1).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t^\\lambda &= (1-\\lambda)\\sum_{n=1}^\\infty{\\lambda^{n-1}G_{t:t+n}}\\\\\\\\\n",
    "&= (1-\\lambda)\\left(G_{t:t+1} + \\sum_{n=2}^\\infty{\\lambda^{n-1}G_{t:t+n}}\\right)\\\\\\\\\n",
    "&= (1-\\lambda)G_{t:t+1} + (1-\\lambda)\\sum_{n=1}^\\infty{\\lambda^{n}\\left(R_{t+1} + \\gamma G_{t+1:t+1+n}\\right)}\\\\\\\\\n",
    "&= (1-\\lambda)G_{t:t+1} + (1-\\lambda)\\sum_{n=1}^\\infty{\\lambda^{n}R_{t+1}} + \\gamma\\lambda(1-\\lambda)\\sum_{n=1}^\\infty{\\lambda^{n-1}G_{t+1:t+n+1}}\\\\\\\\\n",
    "&= (1-\\lambda)G_{t:t+1} + \\lambda R_{t+1} + \\gamma \\lambda G_{t+1}^\\lambda\\\\\\\\\n",
    "&= (1-\\lambda)R_{t+1} + (1-\\lambda)\\gamma \\hv{t+1}{t} + \\lambda R_{t+1} + \\gamma \\lambda G_{t+1}^\\lambda\\\\\\\\\n",
    "&= R_{t+1} + \\gamma\\hv{t+1}{t} + \\gamma\\lambda\\left(G_{t+1}^\\lambda - \\hv{t+1}{t}\\right)\\\\\\\\\n",
    "&= R_{t+1} + \\gamma\\left((1-\\lambda)\\hv{t+1}{t} + \\lambda G_{t+1}^\\lambda\\right)\\\\\\\\\n",
    "&= G_{t:t+1} + \\gamma\\lambda\\left(G_{t+1}^\\lambda - \\hv{t+1}{t}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> _Exercise 12.2_ The parameter $\\lambda$ characterizes how fast the exponential weighting in Figure 12.2 falls off, and thus how far into the future the $\\lambda$-return algorithm looks in determining its update. But a rate factor such as $\\lambda$ is sometimes an awkward way of characterizing the speed of the decay. For some purposes it is better to specify a time constant, or half-life. What is the equation relating $\\lambda$ and the half-life, $\\tau_\\lambda$, the time by which the weighting sequence will have fallen to half of its initial value?\n",
    "\n",
    "We define $\\tau_\\lambda$ such that $(1-\\lambda)\\lambda^{\\tau_\\lambda} = \\frac{1}{2}(1-\\lambda)$. Therefore we have\n",
    "\n",
    "$$\n",
    "\\tau_\\lambda = -\\frac{\\ln{2}}{\\ln{\\lambda}}\n",
    "$$\n",
    "\n",
    "For $n > \\tau_\\lambda + 1$ we would have $(1 - \\lambda)\\lambda^{n-1} < \\frac{1}{2}(1-\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 $\\td{\\lambda}$\n",
    "\n",
    "$$\n",
    "\\delta_t \\quad \\dot{=}\\quad R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\w_t) - \\hat{v}(S_t, \\w_t)\n",
    "$$\n",
    "\n",
    "> _Exercise 12.3_ Some insight into how $\\td{\\lambda}$ can closely approximate the off-line $\\lambda$-return algorithm can be gained by seeing that the latter's error term (in brackets in (12.4)) can be written as the sum of TD errors (12.6) for a single fixed $\\w$. Show this, following the pattern of (6.6), and using the recursive relationship for the $\\lambda$-return you obtained in Exercise 12.1.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t^\\lambda - \\hat{v}(S_t, \\w_t) &= G_{t:t+1} + \\gamma\\lambda\\left(G_{t+1}^\\lambda - \\hv{t+1}{t}\\right) - \\hv{t}{t}\\\\\\\\\n",
    "&= \\delta_t + \\gamma\\lambda\\left(G_{t+1}^\\lambda - \\hv{t+1}{t}\\right)\\\\\\\\\n",
    "&= \\delta_t + \\gamma\\lambda\\left(\\delta_{t+1} + \\gamma\\lambda\\left(G_{t+2}^\\lambda - \\hv{t+2}{t}\\right)\\right)\\\\\\\\\n",
    "&= \\delta_t + \\gamma\\lambda\\delta_{t+1} + \\left(\\gamma\\lambda\\right)^2\\delta_{t+2} + \\dots\\\\\n",
    "&= \\sum_{n=0}^\\infty{\\left(\\gamma\\lambda\\right)^n\\delta_{t+n}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Exercise 12.4 Use your result from the preceding exercise to show that, if the weight updates over an episode were computed on each step but not actually used to change the weights ($\\w$ remained fixed), then the sum of $\\td{\\lambda}$'s weight updates would be the same as the sum of the off-line $\\lambda$-return algorithm's updates.\n",
    "\n",
    "First, note that\n",
    "\n",
    "$$\n",
    "\\z_t = \\sum_{n=0}^t{\\left(\\gamma\\lambda\\right)^n\\nabla\\hv{t-n}{{t-n}}}\n",
    "$$\n",
    "\n",
    "Now, we can see that the sum of the weight updates for $\\td{\\lambda}$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha\\sum_{t=0}^\\infty{\\delta_t\\z_t} &= \\alpha\\sum_{t=0}^\\infty{\\delta_t\\sum_{n=0}^t{\\left(\\gamma\\lambda\\right)^n\\nabla\\hv{t-n}{{t-n}}}}\\\\\\\\\n",
    "&= \\alpha\\sum_{n\\le t}{\\delta_t\\left(\\gamma\\lambda\\right)^n\\nabla\\hv{t-n}{{t-n}}}\\\\\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And for off-line $\\lambda$-return, the sum of weight updates would be\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha\\sum_{t=0}^\\infty{\\left[G_t^\\lambda - \\hv{t}{t}\\right]\\nabla\\hv{t}{t}} &= \\alpha\\sum_{t=0}^\\infty{\\left[\\sum_{n=0}^\\infty{\\left(\\gamma\\lambda\\right)^n\\delta_{t+n}}\\right]\\nabla\\hv{t}{t}}\\\\\\\\\n",
    "&= \\alpha\\sum_{t=0}^\\infty{\\sum_{n=0}^\\infty{\\left(\\gamma\\lambda\\right)^n\\delta_{t+n}}\\nabla\\hv{t}{t}}\\\\\\\\\n",
    "&= \\alpha\\sum_{n\\le t'}^\\infty{\\left(\\gamma\\lambda\\right)^n\\delta_{t'}}\\nabla\\hv{t'-n}{{t'}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "G_{t:h}^\\lambda &\\dot = (1-\\lambda) \\sum_{n=1}^{h-t-1} \\lambda ^{n-1} G_{t:t+n} + \\lambda^{h-t-1} G_{t:h}, \\hspace{5mm} 0 \\leq t < h \\leq T \\tag{12.9} \\\\\n",
    "G_{t:k}^\\lambda &= \\hat v(S_t, \\mathbf{w}_{t-1}) + \\sum_{i=t}^{t+k-1} (\\gamma \\lambda)^{i-t} \\delta_i ^\\prime \\tag{12.10}\\\\\n",
    "\\delta_t & \\dot = R_{t+1} + \\gamma \\hat v(S_{t+1}, \\mathbf{w}_t) - \\hat v(S_t, \\mathbf{w}_{t-1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Also note that from (12.9) we can conclude:\n",
    "$G_{t+1:h}^\\lambda = (1-\\lambda) \\sum_{n=1}^{h-t-2} \\lambda ^{n-1} G_{t+1:t+1+n} + \\lambda^{h-t-2} G_{t+1:h}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_{t:h}^\\lambda &= (1-\\lambda) \\sum_{n=1}^{h-t-1} \\lambda ^{n-1} (R_{t+1} + \\gamma G_{t+1:t+n}) + \\lambda^{h-t-1} G_{t:h}\\\\\n",
    " &= (1-\\lambda) \\left [ R_{t+1} \\sum_{n=1}^{h-t-1} \\lambda ^{n-1}  + \\gamma \\hat v(S_{t+1}, \\mathbf{w}) + \\gamma \\sum_{n=2}^{h-t-1} \\lambda ^{n-1} G_{t+1:t+n} \\right ] + \\lambda^{h-t-1} G_{t:h} \\tag{separating sum}\\\\\n",
    " &= (1-\\lambda) \\left [ R_{t+1} \\frac{\\lambda^{h-t-1} - 1}{\\lambda - 1}  + \\gamma \\hat v(S_{t+1}, \\mathbf{w}) + \\gamma \\sum_{n=2}^{h-t-1} \\lambda ^{n-1} G_{t+1:t+n} \\right ] + \\lambda^{h-t-1} G_{t:h} \\tag{simplifying sum}\\\\\n",
    " &= R_{t+1}(1 - \\lambda^{h-t-1}) + \\gamma (1-\\lambda) \\left [ \\hat v(S_{t+1}, \\mathbf{w}) + \\sum_{n=2}^{h-t-1} \\lambda ^{n-1} G_{t+1:t+n} \\right ] + \\lambda^{h-t-1} G_{t:h}\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise 12.6_ Modify the pseudocode for Sarsa($\\lambda$) to use dutch traces (12.11) without the other features of a true online algorithm. Assume linear function approx. and binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " -1\n",
       "  0\n",
       "  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "𝓐 = [-1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init (generic function with 1 method)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init() = (rand() * 0.2) - 0.6, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step(s, a) = clamp(s[1] + s[2], -1.2, 0.5), clamp(s[2] + 0.001 * a - 0.0025cos(3s[1]), -0.07, 0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_terminal (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_terminal(s) = s[1] < -1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "𝓕 (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function 𝓕(s,a)\n",
    "    tiles_x = [ (l, l +0.2) for l in -1.2:0.2:0.5 ]\n",
    "    tiles_xp = [ (l, l + 0.01) for l in -0.07:0.01:0.07 ]\n",
    "    tiles = cat(tiles_x, tiles_xp, dims=1)\n",
    "    x = [1]\n",
    "    for (i, t) in enumerate(tiles_x)\n",
    "        if t[1] ≤ s[1] ≤ t[2]\n",
    "            append!(x, i)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for (i, t) in enumerate(tiles_xp)\n",
    "        if t[1] ≤ s[2] ≤ t[2]\n",
    "            append!(x, i + length(tiles_x))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    if a == -1\n",
    "        append!(x, x[end] + 1)\n",
    "    elseif a == 0\n",
    "        append!(x, x[end] + 2)\n",
    "    else\n",
    "        append!(x, x[end] + 3)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Int64}:\n",
       "  1\n",
       "  4\n",
       " 16\n",
       " 17\n",
       " 20"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "𝓕(init(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sarsa_λ (generic function with 2 methods)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sarsa_λ(is_terminal, γ, 𝓐, d, init, step, 𝓕; α=0.01, λ=0.5, ε=0.0001)\n",
    "    w = zeros(d)\n",
    "    q(s,a) = sum(w[i] for i in 𝓕(s, a))\n",
    "    function x_(s,a)\n",
    "        xp = zeros(d)\n",
    "        for i in 𝓕(s, a)\n",
    "            xp[i] = 1\n",
    "        end\n",
    "        return xp\n",
    "    end\n",
    "    z = zeros(d)\n",
    "    s = init()\n",
    "    a = argmax(a -> q(s, a), 𝓐)\n",
    "    x = x_(s, a)\n",
    "\n",
    "    while true\n",
    "        r, sp = step(s, a)\n",
    "        δ = r\n",
    "        c = z' * x\n",
    "        for i in 𝓕(s,a)\n",
    "            δ -= w[i]\n",
    "            z[i] += (1 - α*γ*λ*c)*x[i]\n",
    "        end\n",
    "        \n",
    "        if is_terminal(sp)\n",
    "            w += α*δ * z\n",
    "            s = init()\n",
    "            a = argmax(a -> q(s, a), 𝓐)\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        ap = argmax(a -> q(s,a), 𝓐)\n",
    "        for i in 𝓕(s, a)\n",
    "            δ += γ * w[i]\n",
    "        end\n",
    "        \n",
    "        w += α * δ * z\n",
    "        z *= λ * γ\n",
    "        s = sp\n",
    "        x = x_(s, a)\n",
    "        a =ap\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "BoundsError",
     "output_type": "error",
     "traceback": [
      "BoundsError",
      "",
      "Stacktrace:",
      " [1] getindex",
      "   @ ./number.jl:98 [inlined]",
      " [2] 𝓕(s::Float64, a::Int64)",
      "   @ Main ./In[95]:13",
      " [3] (::var\"#x_#109\"{Int64, typeof(𝓕)})(s::Float64, a::Int64)",
      "   @ Main ./In[96]:6",
      " [4] sarsa_λ(is_terminal::typeof(is_terminal), γ::Float64, 𝓐::Vector{Int64}, d::Int64, init::typeof(init), step::typeof(step), 𝓕::typeof(𝓕); α::Float64, λ::Float64, ε::Float64)",
      "   @ Main ./In[96]:40",
      " [5] sarsa_λ(is_terminal::Function, γ::Float64, 𝓐::Vector{Int64}, d::Int64, init::Function, step::Function, 𝓕::Function)",
      "   @ Main ./In[96]:1",
      " [6] top-level scope",
      "   @ In[98]:1",
      " [7] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "sarsa_λ(is_terminal, 0.99, 𝓐, 20, init, step, 𝓕)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise 12.7_ Generalize the three recursive equations above to their truncated versions, defining $G^{\\lambda s}_{t:h}$ and $G^{\\lambda a}_{t:h}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G ^{\\lambda s} _{t:h} &= \n",
    "    R_{t+1} + \\gamma_{t + 1}\n",
    "    \\left(\n",
    "    (1 - \\lambda_t)\\hat{v}(S_{t+1}, \\w_t) + \\lambda_{t+1}G^{\\lambda s}_{t+1:h}\n",
    "    \\right)\\\\\\\\\n",
    "G^{\\lambda s}_{h:h} &= \\left(1 - \\sum_{n=1}^h\\prod_{i=1}^n{\\lambda_i}\\right)G_{t:h}\\\\\\\\\n",
    "G ^{\\lambda a} _{t:h} &= \n",
    "    R_{t+1} + \\gamma_{t + 1}\n",
    "    \\left(\n",
    "    (1 - \\lambda_t)\\hat{q}(S_{t+1}, A_{t+1}\\w_t) + \\lambda_{t+1}G^{\\lambda s}_{t+1:h}\n",
    "    \\right)\\\\\\\\\n",
    "G^{\\lambda a}_{h:h} &= \\left(1 - \\sum_{n=1}^h\\prod_{i=1}^n{\\lambda_i}\\right)G_{t:h}\\\\\\\\\n",
    "G' ^{\\lambda a} _{t:h} &= \n",
    "    R_{t+1} + \\gamma_{t + 1}\n",
    "    \\left(\n",
    "    (1 - \\lambda_t)\\bar{V}(S_{t+1}, \\w_t) + \\lambda_{t+1}G^{\\lambda s}_{t+1:h}\n",
    "    \\right)\\\\\\\\\n",
    "G'^{\\lambda a}_{h:h} &= \\left(1 - \\sum_{n=1}^h\\prod_{i=1}^n{\\lambda_i}\\right)G_{t:h}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with $\\bar{V}$ as defined in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise 12.8_ Prove that (12.24) becomes exact if the value function does not change. To save writing, consider the case of $t = 0$, and use the notation $V_k \\;\\dot{=}\\; \\hat{v}(S_k, \\w)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t^{\\lambda s} &= \\rho_t\\left(R_{t+1} + \\gamma_{t+1}\\left((1-\\lambda_{t+1})V_{t+1} + \\lambda_{t+1}G^{\\lambda s}_{t+1}\\right)\\right) + (1 - \\rho_t)V_t\\\\\\\\\n",
    "&= \\rho_t R_{t+1} + \\rho_t \\gamma_{t+1}(1-\\lambda_{t+1})V_{t+1} + \\rho_t\\gamma_{t+1}\\lambda_{t+1}G^{\\lambda s}_{t+1} + V_t - \\rho_t V_t\\\\\\\\\n",
    "&= \\rho_t R_{t+1} + \\rho_t \\gamma_{t+1}V_{t+1} - \\rho_t \\gamma_{t+1}\\lambda_{t+1}V_{t+1} + \\rho_t\\gamma_{t+1}\\lambda_{t+1}G^{\\lambda s}_{t+1} + V_t - \\rho_t V_t\\\\\\\\\n",
    "G_t - V_t&= \\rho_t\\delta_t - \\rho_t\\gamma_{t+1}\\lambda_{t+1}V_{t+1} + \\rho_t\\gamma_{t+1}\\lambda_{t+1}G_{t+1}\\\\\\\\\n",
    "&= \\rho_t\\left(\\delta_t + \\gamma_{t+1}\\lambda_{t+1}\\left(G_{t+1} - V_{t+1}\\right)\\right)\\\\\\\\\n",
    "&= \\rho_t\\left(\\delta_t + \\gamma_{t+1}\\lambda_{t+1}\\left(\\rho_{t+1}\\left(\\delta_{t+1} + \\gamma_{t+2}\\lambda_{t+2}\\left(G_{t+2} - V_{t+2}\\right)\\right)\\right)\\right)\\\\\\\\\n",
    "&= \\rho_t\\left(\\sum_{k=t}^\\infty{\\delta_k\\prod_{i=t+1}^k{\\gamma_i\\lambda_i\\rho_i}}\\right)\\\\\\\\\n",
    "G_t &= V_t + \\rho_t\\left(\\sum_{k=t}^\\infty{\\delta_k\\prod_{i=t+1}^k{\\gamma_i\\lambda_i\\rho_i}}\\right)\\\\\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\delta_t = R_{t+1} + \\gamma_{t+1}V_{t+1} - V_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise 12.9_ The truncated version of the general off-policy return is denoted $G^{\\lambda s}_{t:h}$.\n",
    "Guess the correct equation, based on (12.24).\n",
    "\n",
    "$$\n",
    "G_t = V_t + \\rho_t\\left(\\sum_{k=t}^{t+h}{\\delta_k\\prod_{i=t+1}^k{\\gamma_i\\lambda_i\\rho_i}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise 12.10_ Prove that (12.27) becomes exact if the value function does not change. To save writing, consider the case of $t = 0$, and use the notation $Q_k = \\hat{q}(S_k, A_k, \\w)$. Hint: Start by writing out $\\delta_0^a$ and $G_0^{\\lambda a}$, then $G_0^{\\lambda a} - Q_0$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "%\n",
    "    \\delta_0 &= R_1 + \\gamma_1\\bar{V}(S_1) - Q_0\\\\\\\\\n",
    "%\n",
    "    G_0 &= R_1 + \\gamma_1\\left(\\bar{V} + \\lambda_{t+1}\\rho_{t+1}\\left[G_1 - Q_1\\right]\\right)\\\\\\\\\n",
    "%\n",
    "%\n",
    "    G_0 - Q_0 &= R_1 + \\gamma_1\\left(\\bar{V} + \\lambda_{t+1}\\rho_{t+1}\\left[G_1 - Q_1\\right]\\right) - Q_0\\\\\\\\\n",
    "    &= R_1 + \\gamma_1\\bar{V} - Q_0 + \\gamma_1 \\lambda_{t+1} \\rho_{t+1}\\left[G_1 - Q_1\\right]\\\\\\\\\n",
    "    &= \\delta_0 + \\gamma_1 \\lambda_{1} \\rho_{1}\\left[G_1 - Q_1\\right]\\\\\\\\\n",
    "    &= \\delta_0 + \\gamma_1 \\lambda_{1} \\rho_{1}\\left(\\delta_1 + \\gamma_2\\lambda_2\\left(G_2 - Q_2\\right)\\right)\\\\\\\\\n",
    "    &= \\sum_{k=0}^{\\infty}{\\delta_k\\left(\\prod_{i=0}^k{\\gamma_i\\lambda_i}\\rho_i\\right)}\\\\\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Exercise 12.11_ The truncated version of the general off-policy return is denoted $G^{\\lambda a}_{t:h}$ Guess the correct equation for it, based on (12.27).\n",
    "\n",
    "$$\n",
    "G_t^{\\lambda a} \\approx \\hat{q}(S_t, A_t, \\mathbf{w}_t) + \\sum_{k=t}^{t+h}{\\delta_k^a \\prod_{i=t+1}^k{\\gamma_i\\lambda_i\\rho_i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exercise 12.12 Show in detail the steps outlined above for deriving (12.29) from (12.27). Start with the update (12.15), substitute $G_t^{\\lambda a}$ from (12.26) for $G_t^\\lambda$, then follow similar steps as led to (12.25).\n",
    "\n",
    "From 12.27 we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{t=0}^\\infty{\\left(\\w_{t+1} - \\w_t\\right)} &= \\alpha \\sum_{t=0}^\\infty{\\sum_{k=t}^\\infty{\\rho_t \\delta_k^a \\prod_{i=t+1}^k{\\gamma_i \\lambda_i \\rho_i}}\\nabla Q_t}\\\\\\\\\n",
    "% \n",
    "&= \\alpha \\sum_{k=0}^\\infty{\\sum_{t=0}^k{\\rho_t \\delta_k^a \\prod_{i=t+1}^k{\\gamma_i \\lambda_i \\rho_i}}\\nabla Q_t}\\\\\\\\\n",
    "% \n",
    "&= \\sum_{k=0}^\\infty{\\alpha \\delta_k \\sum_{t=0}^k{\\rho_t \\nabla Q_t\\prod_{i=t+1}^k{\\gamma_i \\lambda_i \\rho_i}}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $\\nabla Q_t = \\nabla \\hat{q}(S_t, A_t, \\w_t)$.\n",
    "\n",
    "Following the earlier derivation, we assume that the inner sum can be written recursively, as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\z_k &= \\sum_{t=0}^k{\\rho_t \\nabla Q_t\\prod_{i=t+1}^k{\\gamma_i \\lambda_i \\rho_i}}\\\\\\\\\n",
    "&= \\sum_{t=0}^{k-1}{\\rho_t \\nabla Q_t\\prod_{i=t+1}^k{\\gamma_i \\lambda_i \\rho_i}} + \\rho_k \\nabla Q_k\\\\\\\\\n",
    "% \n",
    "&= \\gamma_k \\lambda_k \\rho_k \\left(\\sum_{t=0}^{k-1}{\\rho_t \\nabla Q_t\\prod_{i=t+1}^{k-1}{\\gamma_i \\lambda_i \\rho_i}}\\right) + \\rho_k \\nabla Q_k\\\\\\\\\n",
    "% \n",
    "&= \\gamma_k \\lambda_k \\rho_k \\z_{k-1} + \\rho_k \\nabla Q_k\\\\\\\\\n",
    "% \n",
    "&= \\rho_k \\left( \\gamma_k \\lambda_k \\z_{k-1} + \\nabla Q_k \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exercise 12.13 What are the dutch-trace and replacing-trace versions of off-policy eligibility traces for state-value and action-value methods?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
