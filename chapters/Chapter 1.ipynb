{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "1. This book represents a _computational_ approach to learning from _interaction_.\n",
    "2. Focus on idealized situations\n",
    "3. Goal directed learning from interaction\n",
    "4. >Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal.\n",
    "5. > actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards.\n",
    "6. > We formalize the problem of reinforcement ... as the optimal control of incompletely-known Markov decision processes\n",
    "7. Reinforcement learning is not supervised learning, nor unsupervised learning.\n",
    "   1. Supervised learning requires labels for all possible situations, obviating the need for delayed credit assignment\n",
    "   2. Unsupervised learning is finding structure in data, which dos not necessarily imply maximization of a reward signal\n",
    "8. Exploratation vs Exploitation\n",
    "   1. Explore and find new high value actions, and better estimate the value of existing actions\n",
    "   2. Exploit existing actions for additional reward\n",
    "9. Reinforcement learning does _not_ require a model of the environment to start with\n",
    "10. Reinforcement learning inspired by biological and psychological models\n",
    "11. Terminology\n",
    "   1. _policy_: Mapping from states to actions\n",
    "      1. Stimulus/response rules\n",
    "      2. Can be simple function / lookup table, or involve computation, search or planning\n",
    "      3. Can be stochastic\n",
    "   2. _reward signal_: A scalar received by the agent after each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of simple TD learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "play_game (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataStructures\n",
    "\n",
    "@enum Tile X O blank\n",
    "\n",
    "Board = Array{Tile, 2}\n",
    "\n",
    "initial_board() = fill(blank, (3, 3))\n",
    "\n",
    "function is_win(board::Board, tile::Tile)\n",
    "    for i in 1:3\n",
    "        if board[i, 1] == board[i, 2] == board[i, 3] == tile\n",
    "            return true\n",
    "        elseif board[1, i] == board[2, i] == board[3, i] == tile\n",
    "            return true\n",
    "        end\n",
    "    end\n",
    "    return (board[1, 1] == board[2, 2] == board[3, 3] == tile) || (board[1,3] == board[2,2] == board[3,1] == tile)\n",
    "end\n",
    "\n",
    "is_cats(board::Board) = isempty(findall(x -> x == blank, board))\n",
    "\n",
    "function play(board::Board, x::Int, y::Int, tile::Tile)\n",
    "    board = copy(board)\n",
    "    board[x, y] = tile\n",
    "    board\n",
    "end\n",
    "\n",
    "function play(board::Board, idx::CartesianIndex{2}, tile::Tile)\n",
    "    board = copy(board)\n",
    "    board[idx] = tile\n",
    "    board\n",
    "end\n",
    "\n",
    "function random_policy(board::Board)\n",
    "    moves = findall(x -> x == blank, board)\n",
    "    rand(moves)\n",
    "end\n",
    "\n",
    "function greedy_policy(board, value, tile; ϵ=1e-2)\n",
    "    if rand() < ϵ\n",
    "        return random_policy(board), false\n",
    "    else\n",
    "        max_move = nothing\n",
    "        max_value = -Inf\n",
    "        for pos in findall(x -> x == blank, board)\n",
    "            next_board = play(board, pos, tile)\n",
    "            max_move, max_value = value[next_board] > max_value ? (pos, value[next_board]) : (max_move, max_value)\n",
    "        end\n",
    "        return max_move, true\n",
    "    end\n",
    "end\n",
    "\n",
    "function default_val(board; tile=X)\n",
    "    if is_win(board, tile) \n",
    "        return 1 \n",
    "    elseif is_win(board, tile == X ? O : X) || is_cats(board)\n",
    "        return 0 \n",
    "    else\n",
    "        return 0.5 \n",
    "    end\n",
    "end\n",
    "\n",
    "function learn_policy(;iter=100, α=0.1, ϵ=1e-2)\n",
    "    value = DefaultDict{Board, Float64}(;passkey=true) do key\n",
    "        default_val(key)\n",
    "    end\n",
    "    for i in 1:iter\n",
    "        board = initial_board()\n",
    "        while true\n",
    "            max_move, greedy = greedy_policy(board, value, X; ϵ=ϵ)\n",
    "            next_board = play(board, max_move, X)\n",
    "            if greedy\n",
    "                value[board] = value[board] + α*(value[next_board] - value[board])\n",
    "            end\n",
    "                \n",
    "            if is_win(next_board, X) || is_cats(next_board)\n",
    "                break\n",
    "            end\n",
    "            board = next_board\n",
    "            \n",
    "            next_board = play(board, random_policy(board), O)\n",
    "            if is_win(next_board, O) || is_cats(next_board)\n",
    "                break\n",
    "            end\n",
    "            board = next_board\n",
    "        end\n",
    "    end\n",
    "    return value\n",
    "end\n",
    "\n",
    "function play_game(value; ϵ=0.0)\n",
    "    board = initial_board()\n",
    "    current_player = X\n",
    "    while true\n",
    "        max_move, _ = greedy_policy(board, value, X; ϵ=ϵ)\n",
    "        next_board = play(board, max_move, X)\n",
    "        if is_win(next_board, X)\n",
    "            return true\n",
    "        elseif is_cats(next_board)\n",
    "            return 0\n",
    "        end\n",
    "        board = next_board\n",
    "            \n",
    "        next_board = play(board, random_policy(board), O)\n",
    "        if is_win(next_board, O) || is_cats(next_board)\n",
    "            return 0\n",
    "        end\n",
    "        board = next_board\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultDict{Matrix{Tile}, Float64, var\"#56#57\"} with 1837 entries:\n",
       "  [O X blank; X O O; X blank X]                 => 0.5\n",
       "  [X X blank; X O O; blank O blank]             => 0.999878\n",
       "  [X X blank; blank blank O; blank O blank]     => 0.999939\n",
       "  [X O X; blank blank blank; O blank blank]     => 0.5\n",
       "  [X blank O; X blank O; blank X blank]         => 0.5\n",
       "  [X blank blank; O X O; X blank O]             => 0.996094\n",
       "  [X O blank; blank O blank; blank X X]         => 0.5\n",
       "  [X blank X; O X O; O blank blank]             => 0.75\n",
       "  [O blank O; X X O; X blank blank]             => 0.5\n",
       "  [blank X O; blank X blank; blank X O]         => 1.0\n",
       "  [X O O; blank O blank; X X X]                 => 1.0\n",
       "  [X blank blank; blank blank X; O blank O]     => 0.5\n",
       "  [X O blank; X O blank; X X O]                 => 1.0\n",
       "  [X blank O; blank O blank; blank blank X]     => 0.5\n",
       "  [blank blank blank; blank X blank; blank X O] => 0.5\n",
       "  [blank X blank; O X blank; blank blank blank] => 0.5\n",
       "  [X blank blank; blank O O; X blank X]         => 0.5\n",
       "  [O X blank; X blank X; blank blank O]         => 0.5\n",
       "  [blank X blank; blank O blank; blank blank X] => 0.5\n",
       "  [blank X blank; O blank blank; blank X blank] => 0.5\n",
       "  [O blank X; X O O; X blank X]                 => 0.5\n",
       "  [X blank X; X X O; O O blank]                 => 0.5\n",
       "  [X X O; O X O; X O X]                         => 1.0\n",
       "  [O blank X; X X blank; blank O blank]         => 0.5\n",
       "  [blank blank O; blank blank X; X blank blank] => 0.5\n",
       "  ⋮                                             => ⋮"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = learn_policy(;iter=200000, α=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87464"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ play_game(agent; ϵ=0.0) for _ in 1:100_000 ]) / 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "_Exercise 1.1: Self-Play_\n",
    "\n",
    "> Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?\n",
    "\n",
    "Consider first the case where we know that the value function for both players has converged to some final value for each state. Then, necessarily, these players are in a Nash-equilibrium.\n",
    "\n",
    "Specifically, we can consider each player's strategy as being fully determined by a vector in lying in the $n$-dimensional hyper cube, where $n$ is the number of valid Tic-Tac-Toe states. Assuming that all updates to the value vector move it in a direction of strictly increasing utility w.r.t the current values of the other strategy, it follows that it would only converge when the two vectors are in a Nash equilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TDLearner\n",
    "    value::DefaultDict{Board, Float64}\n",
    "    α::Float64\n",
    "    ϵ::Float64\n",
    "    tile::Tile\n",
    "end\n",
    "\n",
    "function (learner::TDLearner)(board::Board)\n",
    "    action, greedy = greedy_policy(board, learner.value, learner.tile; ϵ=learner.ϵ)\n",
    "    next_board = play(board, action, learner.tile)\n",
    "    if greedy\n",
    "        learner.value[board] = learner.value[board] + learner.α*(learner.value[next_board] - learner.value[board])\n",
    "    end\n",
    "    return next_board\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learn_self_play (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function learn_self_play(;iter=1000, α=0.1, ϵ=1e-2)\n",
    "    x_value = DefaultDict{Board, Float64}(;passkey=true) do key\n",
    "        default_val(key)\n",
    "    end\n",
    "    o_value = DefaultDict{Board, Float64}(;passkey=true) do key\n",
    "        default_val(key)\n",
    "    end\n",
    "    learner_x = TDLearner(x_value, α, ϵ, X)\n",
    "    learner_o = TDLearner(o_value, α, ϵ, O)\n",
    "    for i in 1:iter\n",
    "        board = initial_board()\n",
    "        while true\n",
    "            next_board = learner_x(board)\n",
    "            if is_win(next_board, X) || is_cats(next_board)\n",
    "                break\n",
    "            end\n",
    "            board = next_board\n",
    "            \n",
    "            next_board = learner_o(board)\n",
    "            if is_win(next_board, O) || is_cats(next_board)\n",
    "                break\n",
    "            end\n",
    "            board = next_board\n",
    "        end\n",
    "    end\n",
    "    return learner_x, learner_o\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TDLearner(DefaultDict{Matrix{Tile}, Float64, var\"#43#45\"}([X X blank; blank blank O; blank blank blank] => 0.5, [X X O; O O blank; X blank X] => 0.5, [X O blank; X X O; O blank blank] => 0.55, [X blank X; blank blank blank; blank blank O] => 0.5, [X blank blank; O X blank; X O blank] => 0.5, [O blank X; X blank blank; O X blank] => 0.5, [X X blank; blank blank blank; blank blank O] => 0.5, [X blank X; O blank blank; blank blank blank] => 0.5, [X blank blank; X X O; O blank blank] => 0.5, [X blank blank; O blank blank; X X O] => 0.5…), 0.1, 0.01, X), TDLearner(DefaultDict{Matrix{Tile}, Float64, var\"#44#46\"}([X blank O; X blank O; blank blank blank] => 0.5, [X O blank; X blank blank; blank O blank] => 0.5, [O O blank; X blank blank; blank blank X] => 0.5, [X O O; O blank blank; X X blank] => 0.5, [X O blank; X X O; O blank blank] => 0.5, [O blank X; X blank blank; blank O blank] => 0.5, [X blank O; O blank blank; blank X blank] => 0.5, [X blank X; O blank blank; blank blank blank] => 0.5, [O O blank; X blank X; blank blank blank] => 0.5, [blank blank blank; blank O blank; blank blank X] => 0.5…), 0.1, 0.01, O))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_x, learner_o = learn_self_play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DefaultDict{Matrix{Tile}, Float64, var\"#44#46\"} with 280 entries:\n",
       "  [X blank O; X blank O; blank blank blank]             => 0.5\n",
       "  [X O blank; X blank blank; blank O blank]             => 0.5\n",
       "  [O O blank; X blank blank; blank blank X]             => 0.5\n",
       "  [X O O; O blank blank; X X blank]                     => 0.5\n",
       "  [X O blank; X X O; O blank blank]                     => 0.5\n",
       "  [O blank X; X blank blank; blank O blank]             => 0.5\n",
       "  [X blank O; O blank blank; blank X blank]             => 0.5\n",
       "  [X blank X; O blank blank; blank blank blank]         => 0.5\n",
       "  [O O blank; X blank X; blank blank blank]             => 0.5\n",
       "  [blank blank blank; blank O blank; blank blank X]     => 0.5\n",
       "  [O X blank; X O blank; O X X]                         => 0.5\n",
       "  [X X blank; O blank blank; blank blank O]             => 0.5\n",
       "  [O X X; X O blank; O X O]                             => 0.0\n",
       "  [X blank blank; blank blank blank; blank blank blank] => 0.5\n",
       "  [O X blank; X O blank; O blank X]                     => 0.5\n",
       "  [O X blank; X blank blank; blank O blank]             => 0.5\n",
       "  [X O blank; O X X; X O O]                             => 0.5\n",
       "  [blank blank blank; X O blank; blank blank blank]     => 0.5\n",
       "  [O blank blank; blank blank blank; blank X blank]     => 0.5\n",
       "  [blank blank blank; blank blank O; blank X blank]     => 0.5\n",
       "  [O O O; X X blank; X blank blank]                     => 0.0\n",
       "  [O X blank; X X blank; O blank blank]                 => 0.5\n",
       "  [O blank blank; X O blank; X blank blank]             => 0.5\n",
       "  [X O X; O blank blank; blank blank blank]             => 0.5\n",
       "  [O X blank; X blank blank; O X blank]                 => 0.5\n",
       "  ⋮                                                     => ⋮"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_o.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise 1.2: Symmetries_\n",
    "\n",
    "> Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the larning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?\n",
    "\n",
    "Rather than having a map of all states to values, the agent could have a map from canonical states to values, where there is one canonical state for each set of states that are symmetric with respect to rotation and flipping.\n",
    "\n",
    "This may improve the learning algorithm, because then we will visit each canonicalized state more frequently during training, and therefore gain a better estimate of it's value.\n",
    "\n",
    "However, if the opponent is not symmetric, then it would make sense for us to continue to disregard symmetries in our value table. This is because if the opponent assigns different values to states that are equivalent, then we could benefit by learning and understanding that behavior.\n",
    "\n",
    "As an example, consider an opponent that always maps the following state to moving to the center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Tile}:\n",
       " X::Tile = 0      blank::Tile = 2  blank::Tile = 2\n",
       " blank::Tile = 2  blank::Tile = 2  blank::Tile = 2\n",
       " blank::Tile = 2  blank::Tile = 2  blank::Tile = 2"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ X blank blank ; blank blank blank ; blank blank blank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while mapping the following state to always moving to the \"opposite\" corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 Matrix{Tile}:\n",
       " blank::Tile = 2  blank::Tile = 2  X::Tile = 0\n",
       " blank::Tile = 2  blank::Tile = 2  blank::Tile = 2\n",
       " blank::Tile = 2  blank::Tile = 2  blank::Tile = 2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ blank blank X ; blank blank blank ; blank blank blank ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the two states being symmetric, the asymmetric agent treats them differently. Therefore, the policy's value should be different between these two positions.\n",
    "\n",
    "_Exercise 1.3: Greedy Play_\n",
    "\n",
    "> Suppose the reinforcement learning player was _greedy_, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?\n",
    "\n",
    "It would likely learn to play worse than a non-greedy player, because it may obtain a high estimate of a particular state, and then never attempt to explore other states, leading it to forgo learning more optimal states.\n",
    "\n",
    "_Exercise 1.4: Learning From Exploration_\n",
    "\n",
    "> Suppose learning updates occurred after _all_ moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a different set of probabilities. What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n",
    "\n",
    "The two sets of probabilities are as follows:\n",
    "\n",
    "1. When we _do not_ update after exploratory moves, we are learning the probability of winning the game in the current state _if we only take greedy moves from now on_.\n",
    "2. When we _do_ update after exploratory moves, we are learning the probability of winning the game in the current state _if we continue to make greedy moves with probability $\\epsilon$_.\n",
    "\n",
    "_Exercise 1.5: Other Improvements_\n",
    "\n",
    "> Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem posed?\n",
    "\n",
    "1. You could attempt to have an agent that planned further ahead, but considered _all_ possible opponent moves. This way, the agent could potentially detect if another move leads, regardless of the actions of the other player, to victory."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
