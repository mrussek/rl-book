{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Policy Methods with Approximation\n",
    "\n",
    "The tabular off-policy methods can extend to semi-gradient methods, but they are not guaranteed to converge robustly. While this can be addressed to a limited extent, off-policy learning with function approximation is still an active research area with many open questions.\n",
    "\n",
    "Recall that off-policy learning seeks to estimate either state or action values for either some fixed policy $\\pi$ or some changing policy, but only via observing data according to some behavior policy $b$.\n",
    "\n",
    "The challenges of off-policy learning can be divided into two components.\n",
    "\n",
    "1. The TD-error estimate must be adjusted based on the different probabilities of taking different actions between $b$ and $\\pi$.\n",
    "\n",
    "2. Much more importantly, the distribution of states observed by the behavior policy $b$ is very different than the distribution of states observed by the target policy $\\pi$..\n",
    "\n",
    "## 11.1 Semi-Gradient Methods\n",
    "\n",
    "It is straightforward to turn the updates of on-policy semi-gradient methods into updates for semi-gradient off-policy methods by recalling the _importance sampling_ ratio:\n",
    "$$\n",
    "\\rho_t = \\rho_{t:t} = \\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}\n",
    "$$\n",
    "\n",
    "For TD(0), The update then becomes:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t'+1} \\quad\\dot{=}\\quad \\mathbf{w}_t + \\alpha\\rho_t\\delta_t\\nabla\\hat{v}(S_t, \\mathbf{w}_{t^\\prime})\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\delta_t$ is defined appropriately for either the continuing differential case or the episodic case:\n",
    "\n",
    "For action values, the Expected Sarsa update is:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t'+1} \\quad\\dot{=}\\quad \\mathbf{w}_t + \\alpha\\delta_t\\nabla\\hat{q}(S_t, \\mathbf{w}_t')\n",
    "$$\n",
    "\n",
    "where $\\delta_t$ is defined as before for Expected SARSA. Note that there is no importance sampling ratio in the Sarsa update, which is explained later.\n",
    "\n",
    "In the multi-step case, we simply have as our importance sampling ratio:\n",
    "$$\n",
    "\\rho_{t+1:t+n} = \\prod_{k=t}^{t+N-1}{\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}}\n",
    "$$\n",
    "\n",
    "\n",
    "1. Convert the equation of $n$-step off-policy TD (7.9) to semi-gradient form. Give accompanying definitions of the return for both the episodic and continuing cases.\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\rho_t \\left[ G_{t:t+n} - \\hat{v}(S_t, \\mathbf{w}_t)\\right]\\nabla\\hat{v}(S_t, \\mathbf{w}_t)\n",
    "$$\n",
    "\n",
    "Where, for the episodic case, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\rho_t &= \\prod_{k=t}^{\\min(T, t + n) - 1}{\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}}\\\\\\\\\n",
    "G_{t:t+n} &= \\sum_{k=t+1}^{\\min(T, t + n) - 1)}{\\gamma^{k-t-1}R_k} + \\gamma^n\\hat{v}(S_{t+n}, \\mathbf{w_t})\\mathbb{1}[t + n < T]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "While in the continuing case we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\rho_t &= \\prod_{k=t}^{t + n - 1}{\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}}\\\\\\\\\n",
    "G_{t:t+n} &= \\sum_{k=t+1}^{t + n - 1}{(R_k - \\bar{R}_{k-1})} + \\hat{v}(S_{t+n}, \\mathbf{w_t})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. Convert the equations of $n$-step $Q(\\sigma)$ (7.11 and 7.17) to semi-gradient form. Give definitions that cover both the episodice and continuing cases.\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\left[ G_{t:t+n} - \\hat{q}(S_t, A_t, \\mathbf{w}_t)\\right]\\nabla\\hat{q}(S_t, A_t, \\mathbf{w}_t))\n",
    "$$\n",
    "\n",
    "Where $G_{t:h}$ is defined in the discounted case as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h &= \\min(T, t + n)\\\\\n",
    "G_{t:h} \\quad&\\dot{=}\\quad R_{t+1} + \\gamma(\\sigma_{t+1}\\rho_{t+1} + (1 - \\sigma_{t+1})\\pi(A_t|S_t))(G_{t+1:h} - \\hat{q}_{h-1}(S_{t+1}, A_{t+1}, \\mathbf{w}_t)) + \\gamma \\bar{v}_{h-1}(S_{t+1})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "And $G_{t:h}$ is defined in the continuing case as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_{t:h} \\quad\\dot{=}\\quad R_{t+1} + (\\sigma_{t+1}\\rho_{t+1} + (1 - \\sigma_{t+1})\\pi(A_t|S_t))(G_{t+1:h} - \\hat{q}_{h-1}(S_{t+1}, A_{t+1}, \\mathbf{w}_t)) - \\bar{R}_t + \\bar{v}(S_{t+1}, \\mathbf{w}) - \\bar{R}_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\bar{v}(s, \\mathbf{w}) = \\sum_{a\\in\\scr{A}(s)}\\pi(a|s)\\hat{q}(s, a, \\mathbf{w})\n",
    "$$\n",
    "\n",
    "and $\\sigma_{t+1} \\in (0, 1)$ is chosen by some procedure at each decision from $1$ to $h$.\n",
    "\n",
    "3. Apply one-step semi-gradient Q-learning to Baird's counterexample and show empirically that its weights diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_learning (generic function with 1 method)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ùì¢ = collect(1:7)\n",
    "ùìê = [:dashed, :solid]\n",
    "Œ≥ = 0.99\n",
    "\n",
    "œÄ_(s, a) = a == :solid ? 1 : 0\n",
    "œÄ_sample(s) = :solid\n",
    "b(s, a) = a == :dashed ? 6/7 : 1/7\n",
    "b_sample(s) = rand(vcat(repeat([:dashed], 6), [:solid]))\n",
    "\n",
    "p_sample(s, a) = (a == :dashed ? rand(1:6) : 7, 0)\n",
    "\n",
    "function x(s)\n",
    "    x = zeros(8)\n",
    "    if s == 7\n",
    "        x[7] = 1\n",
    "        x[8] = 2\n",
    "    else\n",
    "        x[s] = 2\n",
    "        x[8] = 1\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "function q_learning(;Œµ=0.1, N=1000, Œ±=0.1)\n",
    "    W = zeros(2, 8)\n",
    "    q(s, a) = a == :dashed ? (W*x)(s)[1] : (W*x(s))[2]\n",
    "    s = rand(ùì¢)\n",
    "    for i in 1:N\n",
    "        a = b_sample(s)\n",
    "        sp, r = p_sample(s, a)\n",
    "        grad = zeros(2, 8)\n",
    "        grad[a == :dashed ? 1 : 0, :] = x(s)\n",
    "        W += Œ±*(R + Œ≥ * maximum(a -> q(sp, a), ùìê)  - q(s, a)) * grad\n",
    "        s = sp\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
