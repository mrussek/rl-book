{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-policy Prediction with Approximation\n",
    "\n",
    "Approximate value function $v_\\pi$ with $\\hat{v}(s, \\mathbf{w}) \\approx v_\\pi(s)$, where $\\mathbf{w} \\in \\mathbb{R}^d$, and $d \\ll |\\mathscr{S}|$. Because we cannot memorize a value for every state, the approximate value function must learn to _generalize_ and learn the value of previously unseen states.\n",
    "\n",
    "Learning a value function makes RL techniques also apply to partially observable problems (POMDPs), since the features available to the value function may only consist of the observable features.\n",
    "\n",
    "However, a value function approximator does not guarantee that the history of the state is accounted for.\n",
    "\n",
    "# 9.1 Value-function Approximation\n",
    "\n",
    "Individual update $s \\mapsto u$. Use supervised learning using update targets $\\{ s_i \\mapsto u_i \\}$ as a dataset.\n",
    "\n",
    "Must be able to learn incrementally, and handle a moving target since $\\pi$ changes with $q_\\pi$\n",
    "\n",
    "# 9.2 The Prediction Objective ($\\mathrm{\\overline{VE}}$)\n",
    "\n",
    "Value function updates affect the prediction for multiple states.\n",
    "\n",
    "It may not be possible to exactly predict the value of every state.\n",
    "\n",
    "Assuming undercapacity to learn exact value function.\n",
    "\n",
    "Specify a distribution over states $\\mu(s)$ $\\sum_s{\\mu(s)} = 1$\n",
    "\n",
    "\n",
    "_Mean squared value error_ : $\\mathrm{\\overline{VE}}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{\\overline{VE}}(\\mathbf{w})& \\; \\dot{=} \\; \\mathbb{E}_{S\\sim\\mu}\\left[\\left(v_\\pi(S) - \\hat{v}(S, \\mathbf{w})\\right)^2\\right]\\\\\n",
    "&= \\sum_{s\\in\\mathscr{S}}{\\mu(s)\\left(v_\\pi(s) - \\hat{v}(s, \\mathbf{w})^2\\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\sqrt{\\mathrm{\\overline{VE}}}$ is a measure of the typical deviation between the true value and the approximated value. $\\mu(s)$ is typically chosen to be or to approximate the fraction of time the agent reaches $s$, and is called the _on-policy distribution_.\n",
    "\n",
    "**The on-policy distribution in episodic tasks**\n",
    "\n",
    "Let the initial state distribution be $S_0 \\sim h$ and $\\eta(s)$ be the average number of time steps spent in state $s$ in a single episode.\n",
    "\n",
    "$$\n",
    "\\eta(s) = h(s) + \\sum_{s'\\in\\mathscr{S}, a\\in\\mathscr{A}(s')}\\eta(s')p(s\\mid a, s') \\pi(a\\mid s')\n",
    "$$\n",
    "\n",
    "This can be thought of as a dynamic programming equation for the number of visits to a state $s$. It can be solved and normalized to give $\\mu$\n",
    "\n",
    "$$\n",
    "\\mu(s) = \\frac{\\eta(s)}{\\sum_{s'}{\\eta(s')}}\n",
    "$$\n",
    "\n",
    "Discounting can be added by multiplying the second term of the recurrence equation for $\\eta(s)$ by $\\gamma$.\n",
    "\n",
    "In continuing tasks, the on-policy distribution is the stationary distribution of states under policy $\\pi$.\n",
    "\n",
    "$$\n",
    "\\mu(s) = \\sum_{s'\\in\\mathscr{S},a\\in\\mathscr{A}(s')}{\\mu(s')\\pi(a\\mid s')p(s \\mid s', a)}\n",
    "$$\n",
    "\n",
    "Ideally, we would want to find a global optimum for $\\mathrm{\\overline{VE}}$, that is, a weight vector $\\mathbf{w}^*$ such that:\n",
    "\n",
    "$$\n",
    "\\mathrm{\\overline{VE}}(\\mathbf{w}^*) \\le \\mathrm{\\overline{VE}}(\\mathbf{w}) \\quad \\mathrm{for\\,all} \\; \\mathbf{w} \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "This is not always possible for complex hypothesis classes. We can however find _local_ optima for these cases. No guarantee for convergence to a global optimum, or even within a bound of the optimum. Can even diverge.\n",
    "\n",
    "Not all function approximation schemes can be considered here, so we limit ourselves to gradient based methods, particularly linear ones.\n",
    "\n",
    "## Stochastic-gradient and Semi-gradient Methods\n",
    "\n",
    "Consider a fixed size weight vector $\\mathbf{w} = \\begin{pmatrix} w_1 & w_2 & w_3 & \\dots & w_d\\end{pmatrix}^T$. Suppose further that $v(s, \\mathbf{w})$ is a differentiable function of $\\mathbf{w}$ at all states $s$. We will be incrementally updating an estimate of $\\mathbf{w}$, starting at some initial estimate $\\mathbf{w}_0$, in order to form the sequence $\\mathbf{w}_t$ for $t = 0, 1, 2, \\dots$. We observe a new sample state $S_t$ on each time step, and it's true value under the current policy. To select $\\mathbf{w}$, we perform stochastic gradient descent as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{w}_{t+1} & \\; \\dot{=} \\; \\mathbf{w}_t - \\frac{1}{2}\\alpha\\nabla_\\mathbf{w}\\left[v_\\pi(S_t) - \\hat{v}(S_t, \\mathbf{w_t})\\right]^2\\\\\n",
    "&= \\mathbf{w}_t + \\alpha\\left(v_\\pi(S_t) - \\hat{v}(S_t, \\mathbf{w}_t)\\right)\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w}_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\nabla_\\mathbf{w} f(\\mathbf{w}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial w_1} & \\frac{\\partial f}{\\partial w_2} & \\dots & \\frac{\\partial f}{\\partial w_d} \\end{pmatrix}\\$\n",
    "\n",
    "We do not attempt zero out the error on the example since we need to balance other states. Additionally, there is no guarantee that the error on this particular state will be made zero by moving in the gradient direction.\n",
    "\n",
    "Stochastic descent only converges when the step-size decreases over time such that:\n",
    "\n",
    "$$\n",
    "\\sum_t{\\alpha_t} = \\infty \\quad \\sum_t{\\alpha_t^2} = 0\n",
    "$$\n",
    "\n",
    "Now consider the case where $v_\\pi(s)$ is unavailable, and instead only one of the bootstrap targets, or an otherwise corrupted form of $v_\\pi(s)$, is available. We call this value $U_t$ as in the earlier section. By substituting $v_\\pi(s)$ with $U_t$, we get the update step for gradient based value estimation:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\left(U_t - \\hat{v}(S_t, \\mathbf{w}_t)\\right)\\nabla_{\\mathbf{w}}{\\hat{v}(S_t, \\mathbf{w})}\n",
    "$$\n",
    "\n",
    "If we select $U_t = G_t$, we get _Gradient Monte-Carlo Value Estimation_.\n",
    "\n",
    "Note however that if we were to have used a bootstrapped estimate such as:\n",
    "\n",
    "$$\n",
    "U_t = R_t + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})\n",
    "$$\n",
    "\n",
    "will depend on the current value of $\\mathbf{w}$ and therefore be _biased estimators_ of $v_\\pi$, and so convergence is not guaranteed. Therefore, gradient bootstrapping methods are _not_ technically gradient descent methods, as our estimator is not guaranteed to have the true value of the gradient as its expectation. They account only for the participation of $\\mathbf{w}$ on the right hand side of the difference, but not for the update in the target. Therefore, they only include part of the gradient and are called _semi-gradient_ methods.\n",
    "\n",
    "Despite this, semi-gradient methods still converge robustly, can be used on continuing problems, and converge more quickly than Monte-Carlo methods for the reasons discussed in Chapter 6.\n",
    "\n",
    "### State aggregation\n",
    "\n",
    "State aggregation assigns one estimated value (and thus a component of $\\mathbf{w}$ to an entire collection of states. So long as the partitioning of states is computable, state aggregation can be used with stochastic gradient descent.\n",
    "\n",
    "**Example: 1000 State Random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state_aggregation_gradient_mc (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function state_aggregation_gradient_mc(;size=1000, N=1, d=10, Œ±=2e-5)\n",
    "    ùíÆ = 1:size\n",
    "    bucket_size = size √∑ d\n",
    "    V = zeros(d)\n",
    "    for i in 1:N\n",
    "        S = Int[]\n",
    "        s = size √∑ 2\n",
    "        println(\"iter $(i)\")\n",
    "        while s in ùíÆ\n",
    "            push!(S, s)\n",
    "            lr = rand() < 0.5 ? -1 : 1\n",
    "            sp = s + lr * rand(1:100)\n",
    "            if sp > 1000\n",
    "                println(\"End right\")\n",
    "                sp = 1001\n",
    "                r = 1\n",
    "            elseif sp < 1\n",
    "                println(\"End left\")\n",
    "                sp = 0\n",
    "                r = -1\n",
    "            end\n",
    "            s = sp\n",
    "        end\n",
    "            \n",
    "        for s in reverse(S)\n",
    "            V[s √∑ bucket_size] += Œ±*(r - V[s √∑ bucket_size])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "End left\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "BoundsError: attempt to access 10-element Vector{Float64} at index [0]",
     "output_type": "error",
     "traceback": [
      "BoundsError: attempt to access 10-element Vector{Float64} at index [0]",
      "",
      "Stacktrace:",
      " [1] getindex",
      "   @ ./array.jl:861 [inlined]",
      " [2] state_aggregation_gradient_mc(; size::Int64, N::Int64, d::Int64, Œ±::Float64)",
      "   @ Main ./In[28]:26",
      " [3] state_aggregation_gradient_mc()",
      "   @ Main ./In[28]:2",
      " [4] top-level scope",
      "   @ In[29]:1",
      " [5] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "state_aggregation_gradient_mc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Methods\n",
    "\n",
    "We consider the case where $\\hat{v}(s, \\mathbf{w})$ is linear in $\\mathbf{w}$. We assume that for each state there is a feature vector $\\mathbf{x}(s) \\in \\mathbb{R}^d$. We can then define:\n",
    "\n",
    "$$\n",
    "\\hat{v}(s, \\mathbf{w})) = \\mathbf{w}^\\top\\mathbf{x}(s)\n",
    "$$\n",
    "\n",
    "In this case the gradient expression is incredibly simple:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}v(s, w) = \\mathbf{x}(s)\n",
    "$$\n",
    "\n",
    "And so we have as our update:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha\\left(U_t - \\mathbf{w}^\\top\\mathbf{x}(s)\\right)\\mathbf{x}(s)\n",
    "$$\n",
    "\n",
    "Guaranteed to converge to a _global_ optimum\n",
    "\n",
    "> _Exercise 9.1_ Show that tabular methods such as presented in Part I of this book are a special case of linear function approximation. What would the feature vectors be?\n",
    "\n",
    "It is a special case of linear function approximation where, for every state $s_i$ \n",
    "\n",
    "$$\n",
    "x_i(s) = \\begin{cases} 1 & s = s_i \\\\ 0 & \\mathrm{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "This works for cases where $|\\mathscr{S}| < \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.5 Feature Construction for Linear Methods\n",
    "\n",
    "> _Exercise 9.2_ Why does (9.17) define $(n+1)^k$ distinct features for dimension $k$?\n",
    "\n",
    "It defines $(n+1)^k$ features because a feature will be defined by a selection of exponents on each of the $k$ state variables. Since there are $n + 1$ available exponents (including zero), the total number of features is $(n + 1)^k$\n",
    "\n",
    "> _Exercise 9.3_ What $n$ and $c_{i,j}$ produce the feature vectors $\\mathbf{x}(s) = \\begin{pmatrix} 1 & s_1 & s_2 & s_1 s_2 & s_1^2 & s_2^2 & s_1 s_2^2 & s_1^2 s_2 & s_1^2 s_2^2 \\end{pmatrix}$\n",
    "\n",
    "$n = 2$ and\n",
    "\n",
    "$$\n",
    "c = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\\\ 2 & 0 \\\\ 0 & 2 \\\\ 2 & 2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "# 9.5.4 Tile Coding\n",
    "\n",
    "> _Exercise 9.4_ Suppose we believe that one of two state dimensions is more likely to have an effect on the value function than is the other, that generalization should be primarily across this dimension than along it. What kind of tilings could be used to take advantage of this prior knowledge?\n",
    "\n",
    "One could use tiles that are asymmetrically elongated along the dimension that we expect to have less of an effect. This increases the scale of generalization along that dimension and leads to the overall value of that feature being learned more quickly but with less precision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
